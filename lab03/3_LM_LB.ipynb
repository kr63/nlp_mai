{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "k1gpzj4guo8e1riwj3om1k",
    "id": "DGYdsAIIyOzR"
   },
   "source": [
    "### N-gram language models or how to write scientific papers\n",
    "\n",
    "Мы обучим нашу языковую модель на статьх [ArXiv](http://arxiv.org/) и посмотрим, сможем ли мы сгенерировать похожую!\n",
    "\n",
    "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
    "\n",
    "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "u8jdaiy68oib3jvr4k01",
    "id": "tVPW6r3ayOzT",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:39:38.227170Z",
     "start_time": "2025-04-07T22:39:38.219376Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "0c76vnyl3zui9yhtkodgrlf",
    "id": "40dc-gvMyOzU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "outputId": "0260e7ab-fb96-4a22-daf0-80231db41f4f",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:39:41.343113Z",
     "start_time": "2025-04-07T22:39:40.809596Z"
    }
   },
   "source": [
    "#!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
    "#!tar -xvzf arxivData.json.tar.gz\n",
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.sample(n=5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  author  day            id  \\\n",
       "15853  [{'name': 'Chenhui Chu'}, {'name': 'Sadao Kuro...    7  1606.02126v4   \n",
       "2544   [{'name': 'Marco Tulio Ribeiro'}, {'name': 'Sa...   16  1602.04938v3   \n",
       "30473  [{'name': 'Ken Sakurada'}, {'name': 'Weimin Wa...    8  1712.02941v1   \n",
       "35383  [{'name': 'Zengyou He'}, {'name': 'Xiaofei Xu'...   24  cs/0505060v1   \n",
       "16436  [{'name': 'Bo Han'}, {'name': 'Will Radford'},...   20  1702.05821v1   \n",
       "\n",
       "                                                    link  month  \\\n",
       "15853  [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
       "2544   [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
       "30473  [{'rel': 'alternate', 'href': 'http://arxiv.or...     12   \n",
       "35383  [{'rel': 'alternate', 'href': 'http://arxiv.or...      5   \n",
       "16436  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
       "\n",
       "                                                 summary  \\\n",
       "15853  As alignment links are not given between Engli...   \n",
       "2544   Despite widespread adoption, machine learning ...   \n",
       "30473  This paper presents a novel method for detecti...   \n",
       "35383  The task of outlier detection is to find small...   \n",
       "16436  Text generation is increasingly common but oft...   \n",
       "\n",
       "                                                     tag  \\\n",
       "15853  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
       "2544   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
       "30473  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "35383  [{'term': 'cs.DB', 'scheme': 'http://arxiv.org...   \n",
       "16436  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
       "\n",
       "                                                   title  year  \n",
       "15853  Supervised Syntax-based Alignment between Engl...  2016  \n",
       "2544   \"Why Should I Trust You?\": Explaining the Pred...  2016  \n",
       "30473  Dense Optical Flow based Change Detection Netw...  2017  \n",
       "35383  A Unified Subspace Outlier Ensemble Framework ...  2005  \n",
       "16436  Post-edit Analysis of Collective Biography Gen...  2017  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15853</th>\n",
       "      <td>[{'name': 'Chenhui Chu'}, {'name': 'Sadao Kuro...</td>\n",
       "      <td>7</td>\n",
       "      <td>1606.02126v4</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>6</td>\n",
       "      <td>As alignment links are not given between Engli...</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Supervised Syntax-based Alignment between Engl...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>[{'name': 'Marco Tulio Ribeiro'}, {'name': 'Sa...</td>\n",
       "      <td>16</td>\n",
       "      <td>1602.04938v3</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>2</td>\n",
       "      <td>Despite widespread adoption, machine learning ...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>\"Why Should I Trust You?\": Explaining the Pred...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30473</th>\n",
       "      <td>[{'name': 'Ken Sakurada'}, {'name': 'Weimin Wa...</td>\n",
       "      <td>8</td>\n",
       "      <td>1712.02941v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>12</td>\n",
       "      <td>This paper presents a novel method for detecti...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Dense Optical Flow based Change Detection Netw...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35383</th>\n",
       "      <td>[{'name': 'Zengyou He'}, {'name': 'Xiaofei Xu'...</td>\n",
       "      <td>24</td>\n",
       "      <td>cs/0505060v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>5</td>\n",
       "      <td>The task of outlier detection is to find small...</td>\n",
       "      <td>[{'term': 'cs.DB', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>A Unified Subspace Outlier Ensemble Framework ...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16436</th>\n",
       "      <td>[{'name': 'Bo Han'}, {'name': 'Will Radford'},...</td>\n",
       "      <td>20</td>\n",
       "      <td>1702.05821v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>2</td>\n",
       "      <td>Text generation is increasingly common but oft...</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Post-edit Analysis of Collective Biography Gen...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "lbyqb5rx7j8jpo591r06ak",
    "id": "fC4Gp_9pyOzU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "47ee23cb-47c6-4478-f707-577804747d15",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:40:06.098127Z",
     "start_time": "2025-04-07T22:40:05.681025Z"
    }
   },
   "source": [
    "# assemble lines: concatenate title and description\n",
    "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7u97m5s8ekl5zd5a43a1yc",
    "id": "koeSWsFtyOzU"
   },
   "source": [
    "### Токенизация\n",
    "\n",
    "Все как обычно, вы уже опытные. Данные кроме тебя никто не токенизирует. Займитесь очисткой данных. Используйте WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "u8rvfk719iek97t3rarwr",
    "id": "SbDUujAdyOzU",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:40:47.285252Z",
     "start_time": "2025-04-07T22:40:44.794786Z"
    }
   },
   "source": [
    "# Task: convert lines (in-place) into strings of space-separated tokens. Import & use WordPunctTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "for i in range(len(lines)):\n",
    "    lines[i] = ' '.join(tokenizer.tokenize(lines[i].lower()))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "w88nddpp2k8edoeyyyjh0l",
    "id": "wpOAF0IfyOzV",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:40:49.138039Z",
     "start_time": "2025-04-07T22:40:49.118122Z"
    }
   },
   "source": [
    "assert sorted(lines, key=len)[0] == \\\n",
    "    'differential contrastive divergence ; this paper has been retracted .'\n",
    "assert sorted(lines, key=len)[2] == \\\n",
    "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qb6h3hxmr095egzv8rlzul",
    "id": "aGL9Ls4tyOzV"
   },
   "source": [
    "### N-граммовая языковая модель\n",
    "\n",
    "Языковая модель - это вероятностная модель, которая оценивает вероятность текста: совместную вероятность всех лексем $w_t$ в тексте $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
    "\n",
    "Это можно сделать, следуя правилу цепочки:\n",
    "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$\n",
    "\n",
    "Проблема такого подхода заключается в том, что финальный термин $P(w_T \\mid w_1, \\dots, w_{T-1})$ зависит от $n-1$ предыдущих слов. Эту вероятность нецелесообразно оценивать для длинных текстов, например, $T = 1000$.\n",
    "\n",
    "Одна из популярных аппроксимаций - предположить, что следующее слово зависит только от конечного количества предыдущих слов:\n",
    "\n",
    "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$.\n",
    "\n",
    "Такая модель называется __n-gram language model__, где n - параметр. Например, в 3-граммовой языковой модели каждое слово зависит только от двух предыдущих слов.\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
    "$$\n",
    "\n",
    "Иногда такую аппроксимацию также можно встретить под названием _марковское предположение n-го порядка_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "u68wydbiioqlp5gl96mhd",
    "id": "L2b40dxVyOzV"
   },
   "source": [
    "Первым этапом построения такой модели является подсчет всех вхождений слов с учетом N-1 предыдущих слов"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "og84gjipnumsakhiiu9ap",
    "id": "rcSMKRpFyOzV",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:41:57.566724Z",
     "start_time": "2025-04-07T22:41:57.553534Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter, deque\n",
    "\n",
    "# special tokens:\n",
    "# - `UNK` represents absent tokens,\n",
    "# - `EOS` is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines, n):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases:\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "\n",
    "    for s in lines:\n",
    "        sentence = s.split()\n",
    "        for i in range(len(sentence)):\n",
    "            prev = []\n",
    "            for j in range(i - 1, i - n, -1):\n",
    "                if j >= 0:\n",
    "                    prev.append(sentence[j])\n",
    "                else:\n",
    "                    prev.append(UNK)\n",
    "            counts[tuple(prev[::-1])][sentence[i]] += 1\n",
    "        prev = []\n",
    "        for j in range(len(sentence) - 1, len(sentence) - n, -1):\n",
    "            if j >= 0:\n",
    "                prev.append(sentence[j])\n",
    "            else:\n",
    "                prev.append(UNK)\n",
    "        counts[tuple(prev[::-1])][EOS] += 1\n",
    "    \n",
    "    return counts\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "xyf2he6lak9mmqarl3nck",
    "id": "jZPOcaJXyOzV",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:42:01.985313Z",
     "start_time": "2025-04-07T22:42:01.952822Z"
    }
   },
   "source": [
    "# let's test it\n",
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
    "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
    "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
    "assert dummy_counts['p', '=']['np'] == 2\n",
    "assert dummy_counts['author', '.']['_EOS_'] == 1"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4j620npeqvj0k8ak8xqx8xk",
    "id": "lT990fFOyOzV"
   },
   "source": [
    "Once we can count N-grams, we can build a probabilistic language model.\n",
    "The simplest way to compute probabilities is in proporiton to counts:\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "c7cm76wmzlaa12bctznzei",
    "id": "IFBR2dVLyOzV",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:42:05.329846Z",
     "start_time": "2025-04-07T22:42:05.320020Z"
    }
   },
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\"\n",
    "        Train a simple count-based language model:\n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "\n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "\n",
    "        # compute token proabilities given counts\n",
    "        self.probs = defaultdict(Counter)\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "\n",
    "        # populate self.probs with actual probabilities\n",
    "        for pair in counts:\n",
    "            denominator = sum(counts[pair].values())\n",
    "            for i in counts[pair]:\n",
    "                self.probs[pair][i] = counts[pair][i] / denominator\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "\n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "0ftnn4nmuzrup6c0vvhb8q",
    "id": "3NdGNXcwyOzW"
   },
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "a7zajcnvhqupvcrmacvkur",
    "id": "MKRW7HyLyOzW",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:42:09.703671Z",
     "start_time": "2025-04-07T22:42:09.676620Z"
    }
   },
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "assert np.allclose(p_initial['learning'], 0.02)\n",
    "assert np.allclose(p_initial['a'], 0.13)\n",
    "assert np.allclose(p_initial.get('meow', 0), 0)\n",
    "assert np.allclose(sum(p_initial.values()), 1)\n",
    "\n",
    "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
    "assert np.allclose(p_a['machine'], 0.15384615)\n",
    "assert np.allclose(p_a['note'], 0.23076923)\n",
    "assert np.allclose(p_a.get('the', 0), 0)\n",
    "assert np.allclose(sum(p_a.values()), 1)\n",
    "\n",
    "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
    "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
    "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
    "    \"your 3-gram model should only depend on 2 previous words\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oh8r9a41kuk4r51wra9",
    "id": "2zLk1Q_CyOzW"
   },
   "source": [
    "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "f17xoejjppmooo2nopw4xo",
    "id": "8TSs8hnHyOzW",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:43:32.699294Z",
     "start_time": "2025-04-07T22:43:12.601333Z"
    }
   },
   "source": [
    "lm = NGramLanguageModel(lines, n=3)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2kd9glwnkr470qc4bt7f1e",
    "id": "mq3ASkyLyOzW"
   },
   "source": [
    "Процесс генерации последовательностей... ну, он последовательный. Вы храните список лексем и итеративно добавляете следующую лексему путем выборки с вероятностями.\n",
    "\n",
    "$ X = [] $\n",
    "\n",
    "__forever:__\n",
    "* $w_{next} \\sim P(w_{next} | X)$\n",
    "* $X = concat(X, w_{next})$\n",
    "\n",
    "\n",
    "Вместо выборки с вероятностями можно также попробовать всегда брать наиболее вероятную лексему, выборку среди топ-K наиболее вероятных лексем или выборку с температурой. В последнем случае (температура) выборка производится из\n",
    "\n",
    "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$.\n",
    "\n",
    "Где $\\tau > 0$ - температура модели. Если $\\tau << 1$, то более вероятные токены будут выбраны с еще большей вероятностью, а менее вероятные исчезнут."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "sgbatlm9vzb4z889fho7",
    "id": "RZSLFp8oyOzW",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:43:53.440141Z",
     "start_time": "2025-04-07T22:43:53.435243Z"
    }
   },
   "source": [
    "def get_next_token(lm, prefix, temperature=1.0):\n",
    "    \"\"\"\n",
    "    return next token after prefix;\n",
    "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "    d = lm.get_possible_next_tokens(prefix)\n",
    "    d = dict(sorted(d.items(), key=lambda item: item[1]))\n",
    "    words = list(d.keys())[::-1]\n",
    "    probs = np.array(list(d.values()))[::-1]\n",
    "    if temperature == 0:\n",
    "        return words[0]\n",
    "    denom = (probs ** (1 / temperature)).sum()\n",
    "    probs = probs ** (1 / temperature) / denom\n",
    "    q = np.random.rand()\n",
    "    i = 0\n",
    "    while q > 0:\n",
    "        q -= probs[i]\n",
    "        i += 1\n",
    "    return words[i - 1]\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "98l40131wjtd5xbdm5b2nr",
    "id": "gcNoEA1AyOzW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2bf388b7-682e-4481-83d1-e049aa51b540",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:44:00.074824Z",
     "start_time": "2025-04-07T22:43:57.567049Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
    "assert 250 < test_freqs['not'] < 450\n",
    "assert 8500 < test_freqs['been'] < 9500\n",
    "assert 1 < test_freqs['lately'] < 200\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
    "assert 1500 < test_freqs['learning'] < 3000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
    "assert 8000 < test_freqs['learning'] < 9000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
    "assert test_freqs['learning'] == 10000\n",
    "\n",
    "print(\"Looks nice!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks nice!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "1nnnycga61rijt6nd8zai",
    "id": "kzYCNpXiyOzX",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:44:03.305510Z",
     "start_time": "2025-04-07T22:44:03.254622Z"
    }
   },
   "source": [
    "prefix = 'native language processing' # <- your ideas :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "\n",
    "print(prefix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native language processing tools . the computational structure w . r . t . the main contribution of a word may actually be achieved , compared to other algorithms by rederiving them in terms of both the link between microscopic and phenomenological models . in this paper is the weakest tongue depending on the caltech pedestrians dataset show that the estimated value function , and the attribution performances obtained from the domains . for quaternions ). already in the corresponding map estimation procedure called gibbs sampling , an application in natural images , providing distributed versions of a digital head phantom obtained with\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "pxyjsv3b7r8thdfxlgitl",
    "id": "Btp7K-TOyOzX",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:44:05.333193Z",
     "start_time": "2025-04-07T22:44:05.297983Z"
    }
   },
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "\n",
    "print(prefix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between the two - sided probing , i . e ., a particular class of models and is capable of handling a large set of binary concept learning in a variety of natural language processing ( nlp ) tasks in environments when many types of annotations for training the network . this paper , we propose a novel approach for capturing the image is a challenging task due to the data . we also consider the problem of estimating the probability of a field - of - the - art approaches . we demonstrate that the proposed method is\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2n90bscmzfko0qnctp7ysc",
    "id": "Q5ZDXt8OyOzX"
   },
   "source": [
    "__Больше в лабе:__ nucleus sampling, top-k sampling, beam search(не для слабонервных)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3gdmey7g8at5n5c5x4gayh",
    "id": "R0L1RWjQyOzX"
   },
   "source": [
    "### Оценка языковых моделей: perplexity (1 балл)\n",
    "\n",
    "_perplexity - это мера того, насколько хорошо ваша модель аппроксимирует истинное распределение вероятностей, лежащее в основе данных. Меньшая perplexity = лучше модель_.\n",
    "\n",
    "Чтобы вычислить недоумение на одном предложении, используйте:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "\n",
    "На уровне корпуса текстов perplexity - это произведение вероятностей всех лексем во всех предложениях в степени $1/N$, где $N$ - _общая длина (в лексемах) всех предложений в корпусе текстов_.\n",
    "\n",
    "Это число может быстро стать слишком маленьким для точности float32/float64, поэтому мы рекомендуем сначала вычислить log-perplexity (из log-probabilities), а затем взять экспоненту."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "5hp010xyzzb4vqewo1bhny",
    "id": "AeZrR7XhyOzX",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:44:08.926796Z",
     "start_time": "2025-04-07T22:44:08.917519Z"
    }
   },
   "source": [
    "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "\n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "\n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "    n = lm.n\n",
    "    final_prob = 0\n",
    "    len_s = 0\n",
    "    for s in lines:\n",
    "        sentence = s.split()\n",
    "        sum_prob = 0\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                prob = lm.get_next_token_prob(' '.join([UNK] * n), sentence[i])\n",
    "                sum_prob += max(np.log(prob), min_logprob) if prob > 0 else min_logprob\n",
    "            elif i < n:\n",
    "                prob = lm.get_next_token_prob(' '.join([UNK] * (n - i) + sentence[0:i]), sentence[i])\n",
    "                sum_prob += max(np.log(prob), min_logprob) if prob > 0 else min_logprob\n",
    "            else:\n",
    "                prob = lm.get_next_token_prob(' '.join(sentence[i - n:i]), sentence[i])\n",
    "                sum_prob += max(np.log(prob), min_logprob) if prob > 0 else min_logprob\n",
    "        i = len(sentence)\n",
    "        if i < n:\n",
    "            prob = lm.get_next_token_prob(' '.join([UNK] * (n - i) + sentence[0:i]), EOS)\n",
    "            sum_prob += max(np.log(prob), min_logprob) if prob > 0 else min_logprob\n",
    "        else:\n",
    "            prob = lm.get_next_token_prob(' '.join(sentence[i - n:i]), EOS)\n",
    "            sum_prob += max(np.log(prob), min_logprob) if prob > 0 else min_logprob\n",
    "        final_prob += sum_prob\n",
    "        len_s += len(sentence) + 1\n",
    "    return np.exp((-final_prob) / len_s)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "8b689bobhkey04x7pabupj",
    "id": "VBrGh3W0yOzX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a99734c8-f3aa-49ba-f235-293b9e97e46a",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:44:18.474441Z",
     "start_time": "2025-04-07T22:44:18.369003Z"
    }
   },
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
    "\n",
    "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be non-negative and reasonably small\"\n",
    "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
    "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
    "    \" Make sure you use min_logprob right\"\n",
    "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ypc4lks4vs1li908fqi8",
    "id": "zE6mU8ypyOzX"
   },
   "source": [
    "Давайте теперь измерим perplexity по факту:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "tjnehsem2lmijkg2lto4w",
    "id": "O6gDt1KyyOzX",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:45:08.941053Z",
     "start_time": "2025-04-07T22:44:24.421732Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1832.23136\n",
      "N = 2, Perplexity = 85653987.28543\n",
      "N = 3, Perplexity = 61999196239911532363776.00000\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "38nfbfkpzgfxik8kccyt1l",
    "id": "hN2QkNQAyOzX"
   },
   "source": [
    "# whoops, it just blew up :)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oopn2o57wxm9vbxzycytce",
    "id": "zo6vRGKMyOzY"
   },
   "source": [
    "### LM Smoothing\n",
    "\n",
    "Проблема нашей простой языковой модели заключается в том, что всякий раз, когда она встречает n-грамму, которую никогда раньше не видела, она присваивает ей вероятность 0. Каждый раз, когда это происходит, недоумение взрывается.\n",
    "\n",
    "Для борьбы с этой проблемой существует техника, называемая __сглаживанием__. Суть ее заключается в том, чтобы изменить подсчеты таким образом, чтобы не допустить слишком низкого значения вероятности. Простейшим алгоритмом здесь является аддитивное сглаживание (оно же [сглаживание Лапаса](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
    "\n",
    "Если количество префиксов невелико, аддитивное сглаживание приведет вероятности к более равномерному распределению. Не стоит забывать, что суммирование в знаменателе идет по _всем словам в словаре_."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "ioh26rlov6g8l2ssj1c8pm",
    "id": "Z2zY2qSOyOzY",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:45:44.763904Z",
     "start_time": "2025-04-07T22:45:44.753885Z"
    }
   },
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel):\n",
    "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "\n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "3xvxkdxcmfqucruyt66mdc",
    "id": "PfkSs1qkyOzY",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:45:57.542950Z",
     "start_time": "2025-04-07T22:45:57.486979Z"
    }
   },
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "j6zqa50koitjjri9ipd8ec",
    "id": "aSoZzBwOyOzY",
    "ExecuteTime": {
     "end_time": "2025-04-07T22:46:53.878521Z",
     "start_time": "2025-04-07T22:45:59.797155Z"
    }
   },
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1832.66878\n",
      "N = 2, Perplexity = 470.48021\n",
      "N = 3, Perplexity = 3679.44765\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "pjuqt30jcerwbz1ym9zv1",
    "id": "Mj1nN58iyOzY"
   },
   "source": [
    "# optional: try to sample tokens from such a model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3b8s1y9uls4fosu3yp28gg",
    "id": "IcZ-M7mwyOzZ"
   },
   "source": [
    "### Сглаживание Кнезера-Нея (4 балла)\n",
    "\n",
    "Сглаживание Лапласа - простой, достаточно хороший метод, но точно не SOTA\n",
    "\n",
    "\n",
    "Ваша последняя задача в этом ноутбуке - реализовать сглаживание [Кнезера-Нея](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing).\n",
    "\n",
    "Оно может быть вычислено рекуррентно, для n>1:\n",
    "\n",
    "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
    "\n",
    "где\n",
    "- $prefix_{n-1}$ - кортеж из {n-1} предыдущих лексем\n",
    "- $lambda_{prefix_{n-1}}$ - константа нормализации, выбранная таким образом, чтобы вероятности складывались в 1\n",
    "- Униграмма $P_{kn}(w_t | prefix_{n-2})$ соответствует сглаживанию Кнезера-Нея для {N-1}-грамматической языковой модели.\n",
    "- Униграмма $P_{kn}(w_t)$ - это частный случай: насколько вероятно увидеть x_t в незнакомом контексте.\n",
    "\n",
    "Более подробные формулы см. на слайдах лекции или в вики.\n",
    "\n",
    "__Ваша задача__ состоит в том, чтобы\n",
    "- реализовать класс `KneserNeyLanguageModel`,\n",
    "- протестировать его на 1-3 грамматических языковых моделях\n",
    "- найти оптимальную (в пределах разумного) дельту сглаживания для 3-граммовой языковой модели со сглаживанием Кнесер-Нея"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "2ix7kzw02v30oye55322all",
    "id": "H6g96hOGyOzZ",
    "ExecuteTime": {
     "end_time": "2025-04-07T23:07:59.975011Z",
     "start_time": "2025-04-07T23:07:59.965777Z"
    }
   },
   "source": [
    "class KneserNeyLanguageModel(NGramLanguageModel):\n",
    "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.lines = lines\n",
    "        self.n = n\n",
    "        self.delta = delta\n",
    "        self.counts = dict()\n",
    "        for index in range(1, self.n + 1):\n",
    "            self.counts[index] = count_ngrams(lines, index)\n",
    "        self.vocab = set(token for token_counts in self.counts[self.n].values() for token in token_counts)\n",
    "        self.probs = self.evaluate_probs(self.n)\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "\n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        \n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "\n",
    "        missing_prob_total = max(0, 1.0 - sum(token_probs.values()))\n",
    "        return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        \n",
    "    def evaluate_probs(self, n):\n",
    "        if n == 1:\n",
    "            probs = defaultdict(Counter)\n",
    "            for prefix in self.counts[1]:\n",
    "                token_counts = self.counts[1][prefix]\n",
    "                total_count = sum(token_counts.values())\n",
    "                lambda_ = self.delta * len(self.vocab) / total_count\n",
    "                probs[prefix] = {token: max(0, token_counts[token] - self.delta) / total_count + lambda_ / len(self.vocab) for token in token_counts}\n",
    "            return probs\n",
    "            \n",
    "        probs = self.evaluate_probs(n-1)\n",
    "        for prefix in self.counts[n]:\n",
    "            token_counts = self.counts[n][prefix]\n",
    "            total_count = sum(token_counts.values())\n",
    "            lambda_ = 0 if n == self.n else self.delta * len(token_counts) / total_count\n",
    "            prev_probs = probs[prefix]\n",
    "            sec = {token: lambda_ * prev_probs[token] for prefix in self.counts[n - 1] for token in token_counts}\n",
    "            probs[prefix] = {token: max(0, token_counts[token] - self.delta) / total_count + sec[token] for token in token_counts}\n",
    "        return probs\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "lsk91832qbmdt7x1q0a8z4",
    "id": "JS4gwQw4yOzZ",
    "ExecuteTime": {
     "end_time": "2025-04-07T23:08:04.172758Z",
     "start_time": "2025-04-07T23:08:03.359270Z"
    }
   },
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "pp3jtkk9annp1qkou58x1b",
    "id": "h5-4AxEtyOzZ",
    "ExecuteTime": {
     "end_time": "2025-04-07T23:08:41.615724Z",
     "start_time": "2025-04-07T23:08:06.259090Z"
    }
   },
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = KneserNeyLanguageModel(train_lines, n=n, delta=1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1832.23136\n",
      "N = 2, Perplexity = 26647.18030\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb",
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
